{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing radar...\n",
      "HDF-PPI-A00-201607200005-B-1080-0450-0010-0015-C.z\n",
      "HDF-PPI-A00-201607200005-B-1080-0450-0010-0015-U.z\n",
      "HDF-PPI-A00-201607200005-B-1080-0450-0010-0040-C.z\n",
      "making /Users/wangzhen/Desktop/docker-gdal/data4/radar_files/HDF-PPI-A00-201607200005-B-1080-0450-0010-0015-C.z\n",
      "HDF-PPI-A00-201607200005-B-1080-0450-0010-0030-U.z\n",
      "making /Users/wangzhen/Desktop/docker-gdal/data4/radar_files/HDF-PPI-A00-201607200005-B-1080-0450-0010-0015-U.z\n",
      "HDF-PPI-A00-201607200005-B-1080-0450-0010-0020-C.z\n",
      "making /Users/wangzhen/Desktop/docker-gdal/data4/radar_files/HDF-PPI-A00-201607200005-B-1080-0450-0010-0040-C.z\n",
      "HDF-PPI-A00-201607200005-B-1080-0450-0010-0030-C.z\n",
      "HDF-PPI-A00-201607200005-B-1080-0450-0010-0020-U.z\n",
      "making /Users/wangzhen/Desktop/docker-gdal/data4/radar_files/HDF-PPI-A00-201607200005-B-1080-0450-0010-0030-U.z\n",
      "making /Users/wangzhen/Desktop/docker-gdal/data4/radar_files/HDF-PPI-A00-201607200005-B-1080-0450-0010-0020-C.z\n",
      "making /Users/wangzhen/Desktop/docker-gdal/data4/radar_files/HDF-PPI-A00-201607200005-B-1080-0450-0010-0030-C.z\n",
      "making /Users/wangzhen/Desktop/docker-gdal/data4/radar_files/HDF-PPI-A00-201607200005-B-1080-0450-0010-0020-U.z\n",
      "432.53925932140555\n",
      "432.53925932140555\n",
      "431.39533013640903\n",
      "431.951382827654\n",
      "432.3761643578764\n",
      "431.951382827654\n",
      "432.3761643578764\n",
      "prov_id is: 118373\n",
      "prov_id is: 118371\n",
      "prov_id is: 118370\n",
      "prov_id is: 118372\n",
      "prov_id is: 118368\n",
      "prov_id is: 118369\n",
      "prov_id is: 118374\n",
      "prov_id is: 118378\n",
      "prov_id is: 118376\n",
      "prov_id is: 118379\n",
      "prov_id is: 118377\n",
      "prov_id is: 118380\n",
      "prov_id is: 118375\n",
      "prov_id is: 118381\n",
      "prov_id is: 118383\n",
      "prov_id is: 118382\n",
      "prov_id is: 118384\n",
      "prov_id is: 118385\n",
      "prov_id is: 118386\n",
      "prov_id is: 118387\n",
      "prov_id is: 118388\n",
      "prov_id is: 118391\n",
      "prov_id is: 118389\n",
      "prov_id is: 118392\n",
      "prov_id is: 118393\n",
      "prov_id is: 118394\n",
      "prov_id is: 118395\n",
      "prov_id is: 118390\n",
      "prov_id is: 118399\n",
      "prov_id is: 118400\n",
      "prov_id is: 118401\n",
      "prov_id is: 118402\n",
      "prov_id is: 118396\n",
      "prov_id is: 118397\n",
      "prov_id is: 118398\n",
      "prov_id is: 118403\n",
      "prov_id is: 118405\n",
      "prov_id is: 118407\n",
      "prov_id is: 118406\n",
      "prov_id is: 118409\n",
      "prov_id is: 118404\n",
      "prov_id is: 118408\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "# Force matplotlib to not use any Xwindows backend.\n",
    "matplotlib.use('Agg')\n",
    "import numpy as np\n",
    "import wradlib as wrl\n",
    "import osgeo.gdal as gdal\n",
    "from prov.dot import prov_to_dot\n",
    "\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "import multiprocessing\n",
    "from PIL import Image\n",
    "from provstore.api import Api\n",
    "\n",
    "import prov.model as prov\n",
    "\n",
    "d1 = prov.ProvDocument()\n",
    "d1.set_default_namespace('http://prov.org/')\n",
    "d1.add_namespace('array2raster','http://array2raster.org/')\n",
    "d1.add_namespace('hdf2', 'http://hdf2filepath.org/')\n",
    "d1.add_namespace('attributes','http://attributes.org/')\n",
    "\n",
    "def array_to_raster(array,grid,raster_file,d2,filename,e_gridded,e_gridxy):\n",
    "    \"\"\"Array > Raster\n",
    "    Save a raster from a C order array.\n",
    "\n",
    "    :param array: ndarray\n",
    "    \"\"\"\n",
    "    e_array=d2.entity('array2raster:e_array',(('filepath',filename),('attributes',\"attributes:array\")))\n",
    "    e_grid=d2.entity('array2raster:e_array',(('filepath',filename),('attributes',\"attributes:array\")))\n",
    "    a_array_shape=d1.activity('hdf2:a_array_shape')\n",
    "    d2.used(a_array_shape,e_array)\n",
    "    d2.wasDerivedFrom(e_array,e_gridded)\n",
    "    d2.wasDerivedFrom(e_grid,e_gridxy)\n",
    "    \n",
    "    x_pixels,y_pixels = array.shape\n",
    "    \n",
    "    e_xpixel=d2.entity('array2raster:e_xpixel',(('filepath',filename),('attributes',\"attributes:xpixel\")))\n",
    "    e_ypixel=d2.entity('array2raster:e_ypixel',(('filepath',filename),('attributes',\"attributes:ypixel\")))\n",
    "    d2.wasGeneratedBy(e_xpixel,a_array_shape)\n",
    "    d2.wasGeneratedBy(e_ypixel,a_array_shape)\n",
    "    \n",
    "    e_size=d2.entity('array2raster:e_size',(('filepath',filename),('attributes',\"attributes:size\")))\n",
    "    e_grid=d2.entity('array2raster:e_grid',(('filepath',filename),('attributes',\"attributes:grid\")))\n",
    "    \n",
    "    PIXEL_SIZE = grid[1,0] - grid[0,0]\n",
    "    \n",
    "    d1.wasDerivedFrom(e_size,e_grid)\n",
    "\n",
    "    print(PIXEL_SIZE)\n",
    "    \n",
    "    e_xmin=d2.entity('array2raster:e_xmin',(('filepath',filename),('attributes',\"attributes:xmin\")))\n",
    "    e_ymax=d2.entity('array2raster:e_ymax',(('filepath',filename),('attributes',\"attributes:ymax\")))\n",
    "    \n",
    "    x_min = grid[0,0]\n",
    "    y_max = grid[0,1]  # x_min & y_max are like the \"top left\" corner.\n",
    "    \n",
    "    d2.wasDerivedFrom(e_xmin,e_grid)\n",
    "    d2.wasDerivedFrom(e_ymax,e_grid)\n",
    "\n",
    "    wkt_projection = 'PROJCS[\"OSGB 1936 / British National Grid\",GEOGCS[\"OSGB 1936\",DATUM[\"OSGB_1936\",SPHEROID[\"Airy 1830\",6377563.396,299.3249646,AUTHORITY[\"EPSG\",\"7001\"]],AUTHORITY[\"EPSG\",\"6277\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.01745329251994328,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4277\"]],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",49],PARAMETER[\"central_meridian\",-2],PARAMETER[\"scale_factor\",0.9996012717],PARAMETER[\"false_easting\",400000],PARAMETER[\"false_northing\",-100000],AUTHORITY[\"EPSG\",\"27700\"],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'\n",
    "    \n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    \n",
    "    a_driver_create=d2.activity('array2raster:a_driver_create')\n",
    "    e_dataset=d2.entity('array2raster:e_dataset',(('filepath',filename),('attributes',\"attributes:dataset\")))\n",
    "    d2.used(a_driver_create,e_xpixel)\n",
    "    d2.used(a_driver_create,e_ypixel)\n",
    "    d2.wasGeneratedBy(e_dataset,a_driver_create)\n",
    "\n",
    "    dataset = driver.Create(\n",
    "        raster_file,\n",
    "        x_pixels,\n",
    "        y_pixels,\n",
    "        1,\n",
    "        gdal.GDT_Float32, )\n",
    "    \n",
    "    \n",
    "    dataset.SetGeoTransform((\n",
    "        x_min,    # 0\n",
    "        PIXEL_SIZE,  # 1\n",
    "        0,                      # 2\n",
    "        y_max,    # 3\n",
    "        0,                      # 4\n",
    "        PIXEL_SIZE))\n",
    "    \n",
    "    a_geotransform=d2.activity('array2raster:a_geotransform')\n",
    "\n",
    "    \n",
    "    d2.used(a_geotransform,e_xmin)\n",
    "    d2.used(a_geotransform,e_ymax)\n",
    "    d2.used(a_geotransform,e_size)\n",
    "    d2.wasGeneratedBy(e_dataset,a_geotransform)\n",
    "    \n",
    "    dataset.SetProjection(wkt_projection)\n",
    "    dataset.GetRasterBand(1).WriteArray(array)\n",
    "    dataset.FlushCache()  # Write to disk.\n",
    "    return dataset, dataset.GetRasterBand(1),raster_file,d1,filename #If you need to return, remenber to return  also the dataset because the band don`t live without dataset.\n",
    "\n",
    "def hdf2array(filename,length):\n",
    "    \n",
    "    e_filename=d1.entity('hdf2:e_filename',(('filepath',filename),('attributes',\"attributes:filename\")))\n",
    "    e_data=d1.entity('hdf2:e_data',(('filepath',filename),('attributes',\"attributes:data\")))\n",
    "    a_read=d1.activity('hdf2:a_read')\n",
    "    \n",
    "    data = wrl.io.read_OPERA_hdf5(filename)\n",
    "    \n",
    "    d1.used(a_read,e_filename)\n",
    "    \n",
    "    d1.wasGeneratedBy(e_data,a_read)\n",
    "    \n",
    "    d1.entity(\"hdf2:e_data\")\n",
    "    d1.activity(\"hdf2:a_read\")\n",
    "    d1.entity(\"hdf2:filename\")\n",
    "    d1.used(\"hdf2:a_read\",\"hdf2:filename\")\n",
    "    d1.used(\"hdf2:e_data\",\"hdf2:a_read\")\n",
    "    \n",
    "    numpy_data =  np.array(data['dataset1/data1/data'][:])\n",
    "    e_numpy_data = d1.entity('hdf2:e_numpy_data')\n",
    "    a_np = d1.activity('hdf2:e_np')\n",
    "    d1.wasGeneratedBy(e_numpy_data,a_np)\n",
    "    e_dbz=d1.entity('hdf2:e_dbz',(('filepath',filename),('attributes',\"attributes:dbz\")))\n",
    "    \n",
    "    d1.activity(\"hdf2:a_np\")\n",
    "    d1.entity(\"hdf2:e_numpy_data\")\n",
    "    d1.used(\"hdf2:a_np\",\"hdf2:e_data\")\n",
    "    d1.wasGeneratedBy(\"hdf2:e_numpy_data\",\"hdf2:a_np\")\n",
    "    \n",
    "    dbZ = (numpy_data * data['dataset1/data1/what']['gain']) +data['dataset1/data1/what']['offset']\n",
    "    dbZ[dbZ < 5] = 0\n",
    "\n",
    "    Z = wrl.trafo.idecibel(dbZ)\n",
    "\n",
    "    e_z=d1.entity('hdf2:e_z',(('filepath',filename),('attributes',\"attributes:z\")))\n",
    "    a_wrl_trafo=d1.activity('hdf2:a_wrl_trafo')\n",
    "    d1.wasGeneratedBy(e_z,a_wrl_trafo)\n",
    "    d1.used(a_wrl_trafo,e_dbz)\n",
    "    \n",
    "    R = wrl.zr.z2r(Z, a=200., b=1.6)\n",
    "    \n",
    "    e_r=d1.entity('hdf2:e_r',(('filepath',filename),('attributes',\"attribute:r\")))\n",
    "\n",
    "    a_wrl_zr=d1.activity('hdf2:a_wrl_zr')\n",
    "    d1.used(a_wrl_zr,e_z)\n",
    "    d1.wasGeneratedBy(e_r,a_wrl_zr)\n",
    "    \n",
    "    depth = wrl.trafo.r2depth(R, 600)\n",
    "    \n",
    "    e_depth=d1.entity('hdf2:e_depth',(('filepath',filename),('attributes',\"attributes:depth\")))\n",
    "    \n",
    "    \n",
    "    d1.used(a_wrl_trafo,e_r)\n",
    "    d1.wasGeneratedBy(e_depth,a_wrl_trafo)\n",
    "\n",
    "    depth[dbZ==0]=0\n",
    "    \n",
    "    d1.wasDerivedFrom(e_depth,e_dbz)\n",
    "\n",
    "    where = data['where']\n",
    "    \n",
    "    e_where=d1.entity('e_where',(('filepath',filename),('attributes',\"attributes:where\")))\n",
    "    d1.wasDerivedFrom(e_where,e_data)\n",
    "\n",
    "    \n",
    "    radar_location = (where['lon'], where['lat'],where['height'])\n",
    "    \n",
    "    e_location=d1.entity('hdf2:e_location',(('filepath',filename),('attributes',\"attributes:location\")))\n",
    "    d1.wasDerivedFrom(e_location,e_where)\n",
    "\n",
    "    \n",
    "    elevation = data['dataset1/where']['elangle']\n",
    "    \n",
    "    e_elevation=d1.entity('hdf2:e_elevation',(('filepath',filename),('attributes',\"attributes:elevation\")))\n",
    "    d1.wasDerivedFrom(e_elevation,e_data)\n",
    "    \n",
    "    azimuths = np.arange(0,360)\n",
    "\n",
    "    e_azimuths=d1.entity('hdf2:e_azimuths',(('filepath',filename),('attributes',\"attributes:azimuths\")))\n",
    "    d1.wasGeneratedBy(e_azimuths,a_np)\n",
    "    \n",
    "    ranges = np.arange(0, length, data['dataset1/where']['rscale']) # in meters\n",
    "    \n",
    "    e_ranges=d1.entity('hdf2:e_ranges',(('filepath',filename),('attributes',\"attributes:ranges\")))\n",
    "    d1.used(a_np,e_data)\n",
    "    d1.wasGeneratedBy(e_ranges,a_np)\n",
    "    \n",
    "    polargrid = np.meshgrid(ranges, azimuths)\n",
    "\n",
    "    e_polargrid=d1.entity('hdf2:e_polargrid',(('filepath',filename),('attributes',\"attributes:polargrid\")))\n",
    "    d1.used(a_np,e_azimuths)\n",
    "    d1.used(a_np,e_ranges)\n",
    "    d1.wasGeneratedBy(e_polargrid,a_np)\n",
    "    \n",
    "    lon, lat, alt = wrl.georef.polar2lonlatalt_n(polargrid[0], polargrid[1],\n",
    "                                                 elevation, radar_location)\n",
    "    \n",
    "    e_lon=d1.entity('hdf2:e_lon',(('filepath',filename),('attributes',\"attributes:lon\")))\n",
    "    e_lat=d1.entity('hdf2:e_lat',(('filepath',filename),('attributes',\"attributes:lat\")))\n",
    "    e_alt=d1.entity('hdf2:e_alt',(('filepath',filename),('attributes',\"attributes:alt\")))\n",
    "    a_wrl_georef=d1.activity('hdf2:wrl_georef')\n",
    "    \n",
    "    d1.used(a_wrl_georef,e_polargrid)\n",
    "    d1.used(a_wrl_georef,e_elevation)\n",
    "    d1.used(a_wrl_georef,e_location)\n",
    "    d1.wasGeneratedBy(e_lon,a_wrl_georef)\n",
    "    d1.wasGeneratedBy(e_lat,a_wrl_georef)\n",
    "    d1.wasGeneratedBy(e_alt,a_wrl_georef)\n",
    "    \n",
    "    bng = wrl.georef.epsg_to_osr(27700)\n",
    "    \n",
    "    e_bng=d1.entity('hdf2:e_bng',(('filepath',filename),('attributes',\"attributes:bng\")))\n",
    "    d1.wasGeneratedBy(e_bng,a_wrl_georef)\n",
    "    \n",
    "    x, y = wrl.georef.reproject(lon, lat, projection_target=bng)\n",
    "    \n",
    "    e_x=d1.entity('hdf2:e_x',(('filepath',filename),('attributes',\"attributes:x\")))\n",
    "    e_y=d1.entity('hdf2:e_y',(('filepath',filename),('attributes',\"attributes:y\")))\n",
    "    d1.used(a_wrl_georef,e_lon)\n",
    "    d1.used(a_wrl_georef,e_lat)\n",
    "    d1.used(a_wrl_georef,e_bng)\n",
    "    d1.wasGeneratedBy(e_x,a_wrl_georef)\n",
    "    d1.wasGeneratedBy(e_y,a_wrl_georef)\n",
    "    \n",
    "    xgrid = np.linspace(x.min(), x.max(), 500)\n",
    "    ygrid = np.linspace(y.min(), y.max(), 500)\n",
    "    \n",
    "    e_xgrid=d1.entity('hdf2:e_xgrid',(('filepath',filename),('attributes',\"attributes:xgrid\")))\n",
    "    e_ygrid=d1.entity('hdf2:e_ygrid',(('filepath',filename),('attributes',\"attributes:ygrid\")))\n",
    "    d1.used(a_np,e_x)\n",
    "    d1.used(a_np,e_y)\n",
    "    d1.wasGeneratedBy(e_xgrid,a_np)\n",
    "    d1.wasGeneratedBy(e_ygrid,a_np)\n",
    "    \n",
    "    grid_xy = np.meshgrid(xgrid, ygrid)\n",
    "    grid_xy = np.vstack((grid_xy[0].ravel(), grid_xy[1].ravel())).transpose()\n",
    "    \n",
    "    e_gridxy=d1.entity('hdf2:e_gridxy',(('filepath',filename),('attributes',\"attributes:gridxy\")))\n",
    "    d1.used(a_np,e_xgrid)\n",
    "    d1.used(a_np,e_ygrid)\n",
    "    d1.wasGeneratedBy(e_gridxy,a_np)\n",
    "    \n",
    "    xy=np.concatenate([x.ravel()[:,None],y.ravel()[:,None]], axis=1)\n",
    "    \n",
    "    e_xy=d1.entity('hdf2:e_xy',(('filepath',filename),('attributes',\"attributes:xy\")))\n",
    "    d1.used(a_np,e_x)\n",
    "    d1.used(a_np,e_y)\n",
    "    d1.wasGeneratedBy(e_xy,a_np)\n",
    "    e_gridded=d1.entity('hdf2:e_gridded',(('filepath',filename),('attributes',\"attributes:gridded\")))\n",
    "    \n",
    "    gridded = wrl.comp.togrid(xy, grid_xy, length, np.array([x.mean(), y.mean()]), depth.ravel(), wrl.ipol.Nearest)\n",
    "    \n",
    "    a_wrl_togrid=d1.activity('hdf2:wrl_togrid')\n",
    "    d1.used(a_wrl_togrid,e_xy)\n",
    "    d1.used(a_wrl_togrid,e_gridxy)\n",
    "    \n",
    "    d1.used(a_wrl_togrid,e_x)\n",
    "    d1.used(a_wrl_togrid,e_y)\n",
    "    d1.used(a_wrl_togrid,e_depth)\n",
    "    d1.wasGeneratedBy(e_gridded,a_wrl_togrid)\n",
    "    \n",
    "    gridded = np.ma.masked_invalid(gridded).reshape((len(xgrid), len(ygrid)))\n",
    "    \n",
    "    d1.used(a_np,e_xgrid)\n",
    "    d1.used(a_np,e_ygrid)\n",
    "    d1.wasGeneratedBy(e_gridded,a_np)\n",
    "    \n",
    "    gridded[gridded.mask] = -999\n",
    "    return gridded,grid_xy,d1,e_gridded,e_gridxy\n",
    "\n",
    "def copy_over_files():\n",
    "    for hdf_file in  glob.glob('/srv/radar/shared_folder/HDF*C.z'):\n",
    "        file_name = hdf_file.split('/')[-1]\n",
    "        new_file = os.path.join('/Users/wangzhen/Desktop/docker-gdal/data4/radar_files',file_name)\n",
    "\n",
    "        if not os.path.exists(new_file):\n",
    "\n",
    "            shutil.copyfile(hdf_file,new_file)\n",
    "            print(hdf_file)\n",
    "\n",
    "\n",
    "def make_tiff(file_path,raster_path,length):\n",
    "    #store the image\n",
    "#     import os\n",
    "    \n",
    "    # visualize the graph\n",
    "#     from prov.dot import prov_to_dot\n",
    "    \n",
    "    \n",
    "    print('making',file_path)\n",
    "    gridded, grid_xy,d1,e_gridded,e_gridxy= hdf2array(file_path, length)\n",
    "    dataset, band,  raster_path,d2,filename = array_to_raster(gridded,grid_xy,raster_path,d1,file_path,e_gridded,e_gridxy)\n",
    "\n",
    "    os.environ[\"PATH\"] += os.pathsep + '/usr/local/Cellar/graphviz/2.40.1/bin'\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    #prov images\n",
    "    dot = prov_to_dot(d2)\n",
    "    png = str(counter)+'.png'\n",
    "    dot.write_png(png)\n",
    "    from IPython.display import Image\n",
    "    Image('prov.png')\n",
    "    counter = counter + 1\n",
    "        \n",
    "    api = Api(username=\"ryan\", api_key=\"70dd9d23b545db0f4baca887f229641f374a3dca\")\n",
    "    provfile = 'provdocument' + str(counter)\n",
    "    stored_document = api.document.create(d2, name=provfile,public = True)\n",
    "    id='prov_id is: '+str(stored_document.id)\n",
    "    print(id)\n",
    "\n",
    "def main_script():\n",
    "    global counter\n",
    "    counter = 0\n",
    "    print('Processing radar...')\n",
    "    files = os.listdir('/Users/wangzhen/Desktop/docker-gdal/data4/radar_tiffs')\n",
    "\n",
    "    for file in files:\n",
    "        try:\n",
    "            Image.open('/Users/wangzhen/Desktop/docker-gdal/data4/radar_tiffs' + file)\n",
    "        except:\n",
    "            print('deleting')\n",
    "            os.remove('/Users/wangzhen/Desktop/docker-gdal/data4/radar_tiffs' + file)\n",
    "\n",
    "    jobs = []\n",
    "\n",
    "    for file in os.listdir('/Users/wangzhen/Desktop/docker-gdal/data4/radar_files'):\n",
    "        print(file)\n",
    "        file_path = os.path.join('/Users/wangzhen/Desktop/docker-gdal/data4/radar_files', file)\n",
    "        b1, b2, b3, dt_string, b4, length, dist, b5, angle, b6 = file.split('-')\n",
    "        length = float(length) * 100\n",
    "        raster_file = '%s_%s_%s.tiff' % (dt_string, int(length), angle)\n",
    "\n",
    "        raster_path = os.path.join('/Users/wangzhen/Desktop/docker-gdal/data4/radar_tiffs', raster_file)\n",
    "        if not os.path.exists(raster_path):\n",
    "            p = multiprocessing.Process(target=make_tiff, args=(file_path,raster_path,length,))\n",
    "            jobs.append(p)\n",
    "            p.start()\n",
    "\n",
    "main_script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
